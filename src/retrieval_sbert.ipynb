{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alaaeddinalia/Desktop/thesis_submission /Rumor_verification/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 : 0.6362\n",
      "Recall@10: 0.7607\n",
      "Recall@15: 0.7607\n",
      "Mean Average Precision (MAP): 0.6635\n"
     ]
    }
   ],
   "source": [
    "from preparing.data_loading import DataLoader\n",
    "from preparing.preprocessor import Preprocessor\n",
    "from preparing.data_cleaning import DataCleaner\n",
    "from utils.preprocessing import preprocess_data\n",
    "from utils.feature_extractor import FeatureExtractor\n",
    "from utils.similarity_calculation import calculate_similarities\n",
    "from evaluation.retrieval_evaluation import evaluate_recall_at_k, evaluate_map\n",
    "from utils.data_split import load_and_combine_datasets, stratified_split\n",
    "import numpy as np\n",
    "\n",
    "train_file = '/Users/alaaeddinalia/Desktop/thesis_submission /Rumor_verification/data/raw/English_train.json'\n",
    "dev_file = '/Users/alaaeddinalia/Desktop/thesis_submission /Rumor_verification/data/raw/English_dev.json'\n",
    "\n",
    "# Combine datasets\n",
    "data = load_and_combine_datasets(train_file, dev_file)\n",
    "\n",
    "\n",
    "#Cleandata\n",
    "cleaner = DataCleaner()\n",
    "clean_data = cleaner.remove_invalid_tweets(data)\n",
    "\n",
    "#Preprocess dataset\n",
    "preprocessor = Preprocessor()\n",
    "preprocessed_data = preprocess_data(clean_data,preprocessor)\n",
    "\n",
    "#data split\n",
    "train_data, test_data = stratified_split(preprocessed_data, label_key='label')\n",
    "\n",
    "#Feature extractor (Sbert)\n",
    "extractor = FeatureExtractor(method=\"sbert\")\n",
    "\n",
    "\n",
    "rumor_texts_test = [item['rumor'] for item in test_data]\n",
    "timeline_texts_test = [timeline_entry[2] for item in test_data for timeline_entry in item['timeline']]\n",
    "\n",
    "\n",
    "\n",
    "rumor_vectors_test = extractor.transform(rumor_texts_test)\n",
    "timeline_vectors_test = extractor.transform(timeline_texts_test)\n",
    "\n",
    "\n",
    "#Assign vectors back to rumors and timeline entries \n",
    "rumor_index = 0\n",
    "timeline_index = 0\n",
    "\n",
    "for item in test_data:\n",
    "    # Assign rumor vector\n",
    "    item['rumor_vector'] = rumor_vectors_test[rumor_index]\n",
    "    rumor_index += 1\n",
    "    \n",
    "    # Assign timeline vectors\n",
    "    for timeline_entry in item['timeline']:\n",
    "        timeline_entry.append(timeline_vectors_test[timeline_index])  \n",
    "        timeline_index += 1\n",
    "        \n",
    "# Calculate similarities between rumors and timeline entries\n",
    "similarities = calculate_similarities(test_data) \n",
    "\n",
    "\n",
    "# Recall at k\n",
    "recall_at_5 = evaluate_recall_at_k(test_data, similarities, 5)\n",
    "recall_at_10 = evaluate_recall_at_k(test_data, similarities, 10)\n",
    "recall_at_15 = evaluate_recall_at_k(test_data, similarities, 15)\n",
    "\n",
    "# MAP\n",
    "map_score = evaluate_map(test_data, similarities)\n",
    "\n",
    "print(f\"Recall@5 : {recall_at_5:.4f}\")\n",
    "print(f\"Recall@10: {recall_at_10:.4f}\")\n",
    "print(f\"Recall@15: {recall_at_15:.4f}\")\n",
    "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alaaeddinalia/Desktop/thesis_submission /Rumor_verification/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 : 0.7778\n",
      "Recall@10: 0.8000\n",
      "Recall@15: 0.8000\n",
      "Mean Average Precision (MAP): 0.7085\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arabic_train_file = '/Users/alaaeddinalia/Desktop/thesis_submission /Rumor_verification/data/raw/Arabic_train.json'\n",
    "arabic_dev_file = '/Users/alaaeddinalia/Desktop/thesis_submission /Rumor_verification/data/raw/Arabic_dev.json'\n",
    "\n",
    "# Combine datasets\n",
    "data = load_and_combine_datasets(arabic_train_file, arabic_dev_file)\n",
    "\n",
    "\n",
    "#Cleandata\n",
    "cleaner = DataCleaner()\n",
    "clean_data = cleaner.remove_invalid_tweets(data)\n",
    "\n",
    "#Preprocess dataset\n",
    "preprocessor = Preprocessor()\n",
    "preprocessed_data = preprocess_data(clean_data,preprocessor)\n",
    "\n",
    "train_data, test_data = stratified_split(preprocessed_data, label_key='label')\n",
    "\n",
    "#Feature extractor (Sbert)\n",
    "extractor = FeatureExtractor(method=\"sbert\")\n",
    "\n",
    "# Prepare test data \n",
    "rumor_texts_test = [item['rumor'] for item in test_data]\n",
    "timeline_texts_test = [timeline_entry[2] for item in test_data for timeline_entry in item['timeline']]\n",
    "\n",
    "\n",
    "\n",
    "rumor_vectors_test = extractor.transform(rumor_texts_test)\n",
    "timeline_vectors_test = extractor.transform(timeline_texts_test)\n",
    "\n",
    "\n",
    "#Assign vectors back to rumors and timeline entries \n",
    "rumor_index = 0\n",
    "timeline_index = 0\n",
    "\n",
    "for item in test_data:\n",
    "    # Assign rumor vector\n",
    "    item['rumor_vector'] = rumor_vectors_test[rumor_index]\n",
    "    rumor_index += 1\n",
    "    \n",
    "    # Assign timeline vectors\n",
    "    for timeline_entry in item['timeline']:\n",
    "        timeline_entry.append(timeline_vectors_test[timeline_index])  \n",
    "        timeline_index += 1\n",
    "        \n",
    "# Calculate similarities between rumors and timeline entries\n",
    "similarities = calculate_similarities(test_data) \n",
    "\n",
    "\n",
    "# Recall at k\n",
    "recall_at_5 = evaluate_recall_at_k(test_data, similarities, 5)\n",
    "recall_at_10 = evaluate_recall_at_k(test_data, similarities, 10)\n",
    "recall_at_15 = evaluate_recall_at_k(test_data, similarities, 15)\n",
    "\n",
    "# MAP\n",
    "map_score = evaluate_map(test_data, similarities)\n",
    "\n",
    "print(f\"Recall@5 : {recall_at_5:.4f}\")\n",
    "print(f\"Recall@10: {recall_at_10:.4f}\")\n",
    "print(f\"Recall@15: {recall_at_15:.4f}\")\n",
    "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "104b1e582622f16709a9992a99be4bbc9dea93f31728655411d1793dc31798ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
